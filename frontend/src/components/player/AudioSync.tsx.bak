'use client';

import { useRef, useEffect, useState, useCallback } from 'react';
import { useAuthStore } from '@/lib/store/authStore';
import { usePlayerStore } from '@/lib/store/playerStore';
import env from '@/utils/env';
import { useAudioWithAuth } from '@/hooks/useAudioWithAuth';
import SSEManager from '@/lib/sse/SSEManager';
import HLSManager from '@/lib/hls/HLSManager';

// Constant for track playback offset (in seconds)
const TRACK_PLAYBACK_OFFSET = 0;

// Settings for drift correction
const DRIFT_SETTINGS = {
  // Threshold below which we don't correct (ms)
  minDriftThreshold: 100,
  // Mobile-specific settings
  mobile: {
    // Longer minimum time between full position checks on mobile (ms)
    minTimeBetweenChecks: 60000, // 1 minute
    // Higher drift threshold before correcting on mobile
    minDriftThreshold: 300, // 300ms
    // Disable crossfade on mobile (more resource intensive)
    disableCrossfade: true,
    // Minimum time between any corrections on mobile (ms)
    correctionCooldown: 10000, // 10 seconds
  }
};

// Singleton audio instance
let globalAudioInstance: HTMLAudioElement | null = null;

export default function AudioSync() {
  const audioRef = useRef<HTMLAudioElement>(null);
  const loadingRef = useRef<boolean>(false);
  const mountedRef = useRef<boolean>(false);
  const startTimeRef = useRef<number>(0);
  const trackIdRef = useRef<string | null>(null);
  const sseManagerRef = useRef<SSEManager | null>(null);
  const statusRef = useRef<string>('stopped');
  const [streamError, setStreamError] = useState<boolean>(false);
  const { token } = useAuthStore();
  const { 
    status, 
    currentTrack,
    setPosition,
    setPlayerState
  } = usePlayerStore();
  
  const audioContextRef = useRef<AudioContext | null>(null);
  const [isInitialized, setIsInitialized] = useState(false);
  const sseSubscriptionRef = useRef(false);
  const trackEndTimerRef = useRef<NodeJS.Timeout | null>(null);
  const wakeLockRef = useRef<any>(null);
  const requestTimeRef = useRef<number>(0); // Track when position requests are made
  const playbackStartTimeRef = useRef<number>(0); // Track when playback actually starts
  const hasInitialSyncRef = useRef<boolean>(false);
  const initialSyncTimeRef = useRef<number>(0); // When we got the server time
  const serverPositionRef = useRef<number>(0);  // Position from server
  const positionSyncInProgressRef = useRef<boolean>(false); // Lock to prevent multiple position requests
  const bufferingCompensationAppliedRef = useRef<boolean>(false); // Track if buffering compensation was applied
  const lastStatusChangeTimeRef = useRef<number>(0); // Track time of last status change
  
  const lastSyncTimeRef = useRef<number>(0);
  const driftCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const isApplyingCorrectionRef = useRef<boolean>(false);
  const originalPlaybackRateRef = useRef<number>(1);
  const crossfadeAudioRef = useRef<HTMLAudioElement | null>(null);
  const driftHistoryRef = useRef<{timestamp: number, drift: number}[]>([]);
  const lastCorrectionTimeRef = useRef<number>(0); // Track last correction time
  const isMobileDeviceRef = useRef<boolean>(false); // Track if we're on a mobile device
  
  useAudioWithAuth();

  // Function to reset sync state when track changes
  const resetSyncState = useCallback(() => {
    hasInitialSyncRef.current = false;
    initialSyncTimeRef.current = 0;
    serverPositionRef.current = 0;
    console.log('Audio: Reset sync state');
  }, []);

  // Function to fetch position from API with latency compensation
  const fetchPositionWithLatencyCompensation = useCallback(async (audio: HTMLAudioElement) => {
    // Prevent multiple simultaneous position requests
    if (positionSyncInProgressRef.current) {
      console.log('Audio: Position sync already in progress, skipping request');
      return { position: audio.currentTime };
    }
    
    try {
      positionSyncInProgressRef.current = true;
      
    // Record the time before making the request
    requestTimeRef.current = performance.now();
    const response = await fetch(`${env.apiUrl}/api/music/position`);
    const data = await response.json();
    
    // Calculate request duration
    const requestDuration = performance.now() - requestTimeRef.current;
    console.log(`Audio: Position request took ${requestDuration.toFixed(2)}ms`);
    
    // Compensate for network delay and apply offset
    const adjustedPosition = Math.max(0, data.position - (requestDuration / 1000) - TRACK_PLAYBACK_OFFSET);
    
    console.log('Audio: Setting position from API', {
      original: data.position,
      adjusted: adjustedPosition,
      compensation: requestDuration / 1000,
      offset: TRACK_PLAYBACK_OFFSET,
      serverTime: data.timestamp
    });
    
    // Store the initial sync data
    hasInitialSyncRef.current = true;
    initialSyncTimeRef.current = performance.now();
    serverPositionRef.current = adjustedPosition;
    
    audio.currentTime = adjustedPosition;
      
      // Reset buffering compensation flag
      bufferingCompensationAppliedRef.current = false;
    
    // Record time before attempting to play
    playbackStartTimeRef.current = performance.now();
    
    const handlePlaybackStart = () => {
        if (bufferingCompensationAppliedRef.current) {
          console.log('Audio: Buffering compensation already applied, skipping');
          return;
        }
        
        bufferingCompensationAppliedRef.current = true;
      const playbackStartDelay = performance.now() - playbackStartTimeRef.current;
      console.log(`Audio: Playback actually started after ${playbackStartDelay.toFixed(2)}ms (includes buffering)`);
      const bufferingCompensation = playbackStartDelay / 1000;
      const newPosition = Math.max(0, audio.currentTime + bufferingCompensation);
      console.log(`Audio: Compensating for buffering delay by seeking forward ${bufferingCompensation.toFixed(3)}s to ${newPosition.toFixed(3)}s`);
      audio.currentTime = newPosition;
    };
    
    audio.addEventListener('playing', handlePlaybackStart, { once: true });
    
    return { position: audio.currentTime };
    } finally {
      // Release the lock after a short delay to prevent immediate re-requests
      setTimeout(() => {
        positionSyncInProgressRef.current = false;
      }, 500);
    }
  }, []);

  // Function to get current position without API call
  const getCalculatedPosition = useCallback(() => {
    if (!hasInitialSyncRef.current) {
      console.log('Audio: No initial sync data available, fetching from server');
      return 0; // Will trigger a fetch from server
    }
    
    const elapsedSinceSync = (performance.now() - initialSyncTimeRef.current) / 1000;
    const calculatedPosition = serverPositionRef.current + elapsedSinceSync;
    
    console.log('Audio: Calculated position', {
      initial: serverPositionRef.current,
      elapsed: elapsedSinceSync,
      calculated: calculatedPosition
    });
    
    return calculatedPosition;
  }, []);

  // Function to set position with buffering compensation
  const setPositionWithBufferingCompensation = useCallback((audio: HTMLAudioElement, position: number) => {
    // Apply the offset when setting position (subtract offset to make playback delayed)
    audio.currentTime = Math.max(0, position - TRACK_PLAYBACK_OFFSET);
    
    // Reset buffering compensation flag
    bufferingCompensationAppliedRef.current = false;
    
    playbackStartTimeRef.current = performance.now();
    
    const handlePlaybackStart = () => {
      if (bufferingCompensationAppliedRef.current) {
        console.log('Audio: Buffering compensation already applied, skipping');
        return;
      }
      
      bufferingCompensationAppliedRef.current = true;
      const playbackStartDelay = performance.now() - playbackStartTimeRef.current;
      console.log(`Audio: Playback actually started after ${playbackStartDelay.toFixed(2)}ms (includes buffering)`);
      const bufferingCompensation = playbackStartDelay / 1000;
      const newPosition = Math.max(0, audio.currentTime + bufferingCompensation);
      console.log(`Audio: Compensating for buffering delay by seeking forward ${bufferingCompensation.toFixed(3)}s to ${newPosition.toFixed(3)}s`);
      audio.currentTime = newPosition;
    };
    
    audio.addEventListener('playing', handlePlaybackStart, { once: true });
  }, []);

  // Function to setup stream for a track
  const setupStream = useCallback(async () => {
    if (!mountedRef.current || !audioRef.current || !currentTrack) return;

    console.log('Audio: Setting up stream', {
      trackId: currentTrack.youtubeId,
      loading: loadingRef.current,
      initialized: isInitialized,
      currentStatus: statusRef.current
    });

    // If already loading, wait for it to complete or timeout
    if (loadingRef.current) {
      console.log('Audio: Another stream setup is in progress, waiting...');
      let retryCount = 0;
      while (loadingRef.current && retryCount < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        retryCount++;
      }
      if (loadingRef.current) {
        console.log('Audio: Timeout waiting for previous stream setup to complete');
        return;
      }
    }
    
    try {
      // Set loading flag first to prevent concurrent setups
      loadingRef.current = true;
      
      // Clear any existing locks
      positionSyncInProgressRef.current = false;
      bufferingCompensationAppliedRef.current = false;
      
      const audio = audioRef.current;
      setStreamError(false);
      
      // Store the track status at the start of setup
      const targetStatus = statusRef.current === 'playing' || usePlayerStore.getState().status === 'playing'
        ? 'playing' : 'paused';
      
      console.log('Audio: Stream setup target status:', targetStatus);
      
      const hlsManager = HLSManager.getInstance();
      // Prefer direct secure streaming over HLS to avoid ffmpeg usage
      // Check for secure streaming support in the browser
      const supportsSecureStreaming = true; // All modern browsers support this
      
      if (supportsSecureStreaming) {
        console.log('Audio: Using secure direct streaming for playback');
        try {
          // First fetch a secure token
          const tokenResponse = await fetch(`${env.apiUrl}/api/music/secure-token/${currentTrack.youtubeId}`);
          if (!tokenResponse.ok) {
            throw new Error('Failed to obtain secure token');
          }
          
          const { token } = await tokenResponse.json();
          const streamUrl = `${env.apiUrl}/api/music/secure-stream/${token}`;
          audio.src = streamUrl;
          audio.load();
        } catch (secureStreamError) {
          console.error('Audio: Secure streaming failed, falling back to HLS:', secureStreamError);
          if (hlsManager.isSupported()) {
            console.log('Audio: Falling back to HLS for playback');
            await hlsManager.attachMedia(audio, currentTrack.youtubeId);
          } else {
            console.log('Audio: Falling back to direct stream URL for playback');
            const streamUrl = `${env.apiUrl}/api/music/stream?ts=${Date.now()}&track=${currentTrack.youtubeId}`;
            audio.src = streamUrl;
            audio.load();
          }
        }
      } else if (hlsManager.isSupported()) {
        console.log('Audio: Using HLS for playback');
        await hlsManager.attachMedia(audio, currentTrack.youtubeId);
      } else {
        console.log('Audio: Using direct stream URL for playback');
        const streamUrl = `${env.apiUrl}/api/music/stream?ts=${Date.now()}&track=${currentTrack.youtubeId}`;
        audio.src = streamUrl;
        audio.load();
      }
      
      // Wait for metadata to load
      await Promise.race([
        new Promise<void>((resolve, reject) => {
          const handleMetadata = () => {
            cleanup();
            console.log('Audio: Metadata loaded successfully');
            resolve();
          };

          const handleError = (error: Event) => {
            cleanup();
            console.error('Audio: Metadata loading error', error);
            reject(error);
          };

          const cleanup = () => {
            audio.removeEventListener('loadedmetadata', handleMetadata);
            audio.removeEventListener('error', handleError);
          };

          audio.addEventListener('loadedmetadata', handleMetadata, { once: true });
          audio.addEventListener('error', handleError, { once: true });
        }),
        new Promise<void>((_, reject) => 
          setTimeout(() => reject(new Error('Metadata loading timeout after 10 seconds')), 10000)
        )
      ]);

      trackIdRef.current = currentTrack.youtubeId;
      console.log('Audio: Stream setup complete for track', currentTrack.youtubeId);

      // Get the current player status again (it might have changed during setup)
      const currentStatus = statusRef.current === 'playing' || usePlayerStore.getState().status === 'playing'
        ? 'playing' : 'paused';
        
      console.log('Audio: Current status after stream setup:', currentStatus);

      // If status is playing, setup auto-play once stream is ready
      if (currentStatus === 'playing') {
        console.log('Audio: Current status is playing, preparing to play after stream setup');
        
        // Setup one-time handler for canplay event
        const handleCanPlay = async () => {
          // Remove the listener first to avoid multiple calls
          audio.removeEventListener('canplay', handleCanPlay);
          console.log('Audio: New track can play, attempting to start playback');
          
          // Check status again - it might have changed while waiting for canplay
          if (statusRef.current === 'playing' || usePlayerStore.getState().status === 'playing') {
            try {
              // Use the position from the server to ensure sync
              // Set the lock to prevent duplicate position sync
              if (!positionSyncInProgressRef.current && !hasInitialSyncRef.current) {
                await fetchPositionWithLatencyCompensation(audio);
              } else {
                console.log('Audio: Position sync already performed, using current position:', audio.currentTime);
              }
              
              // Only play if we're still in a playing state
              if (statusRef.current === 'playing' || usePlayerStore.getState().status === 'playing') {
                await audio.play().catch(e => {
                  console.error('Audio: Failed to play new track after setup:', e);
                });
              } else {
                console.log('Audio: Not starting playback - status changed to', statusRef.current);
              }
            } catch (error) {
              console.error('Audio: Error starting playback for new track after setup:', error);
            }
          } else {
            console.log('Audio: Track ready but not starting playback - player state is now', statusRef.current);
          }
        };
        
        // Register the canplay handler
        audio.addEventListener('canplay', handleCanPlay, { once: true });
      } else {
        console.log('Audio: Current status is not playing, skipping auto-play setup');
      }
    } catch (error) {
      console.error('Audio: Stream setup error:', error);
      setStreamError(true);
    } finally {
      // Short delay before releasing loading lock to prevent immediate retriggering
      setTimeout(() => {
      if (mountedRef.current) {
        loadingRef.current = false;
          console.log('Audio: Stream setup loading flag released');
      }
      }, 100);
    }
  }, [currentTrack, isInitialized, fetchPositionWithLatencyCompensation]);

  const initializeAudioContext = useCallback(async () => {
    if (!audioContextRef.current) {
      const AudioContext = window.AudioContext || (window as any).webkitAudioContext;
      audioContextRef.current = new AudioContext();
    }
    return audioContextRef.current;
  }, []);

  const playAudio = async (audio: HTMLAudioElement) => {
    try {
      if (!audio.paused) {
        console.log('Audio: Already playing');
        return;
      }

      // Initialize audio context and play
      const audioContext = await initializeAudioContext();
      if (audioContext.state === 'suspended') {
        await audioContext.resume();
      }

      // Position is handled by loadeddata event
      // Just start playback
      console.log('Audio: Starting playback from position:', audio.currentTime);
      const playPromise = audio.play();
      if (playPromise !== undefined) {
        await playPromise;
        console.log('Audio: Playback started successfully');
      }
    } catch (error) {
      console.error('Audio: Playback failed:', error);
      if (error instanceof DOMException && error.name === 'NotAllowedError') {
        console.log('Audio: Playback blocked by browser, waiting for user interaction');
        // Add user interaction handler
        return new Promise<void>((resolve) => {
          const handleInteraction = async () => {
            cleanup();
            try {
              const audioContext = await initializeAudioContext();
              if (audioContext.state === 'suspended') {
                await audioContext.resume();
              }
              await audio.play();
              resolve();
            } catch (retryError) {
              console.error('Audio: Retry play failed:', retryError);
            }
          };

          const cleanup = () => {
            document.removeEventListener('click', handleInteraction);
            document.removeEventListener('touchstart', handleInteraction);
          };

          document.addEventListener('click', handleInteraction);
          document.addEventListener('touchstart', handleInteraction);
        });
      }
    }
  };

  // Status change effect - optimized with rate limiting
  useEffect(() => {
    if (!audioRef.current || !mountedRef.current || !isInitialized) return;

    // Rate limit status changes to prevent rapid consecutive triggers
    const now = performance.now();
    const timeSinceLastChange = now - lastStatusChangeTimeRef.current;
    if (timeSinceLastChange < 300) { // 300ms debounce
      console.log('Audio: Status change throttled (too frequent)', { 
        status, 
        timeSinceLastChange: `${timeSinceLastChange.toFixed(2)}ms`,
        minInterval: '300ms'
      });
      return;
    }
    
    lastStatusChangeTimeRef.current = now;

    const audio = audioRef.current;
    console.log('Audio: Status change effect triggered', { 
      status, 
      isPaused: audio.paused,
      isInitialized,
      currentTrack: currentTrack?.youtubeId,
      hasInitialSync: hasInitialSyncRef.current
    });

    // Don't attempt to play if we're loading a new track
    if (loadingRef.current) {
      console.log('Audio: Loading in progress, deferring playback');
      return;
    }

    // Skip if status and audio state are already aligned
    if ((status === 'playing' && !audio.paused) || (status === 'paused' && audio.paused)) {
      console.log('Audio: Status already aligned with desired state');
      return;
    }

    const startPlayback = async () => {
      // When starting playback, reset the buffering compensation flag
      bufferingCompensationAppliedRef.current = false;
      
      try {
        const audioContext = await initializeAudioContext();
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }

        await audio.play().catch(error => {
          console.error('Audio: Play error in startPlayback:', error);
          if (error.name !== 'NotAllowedError') {
            setPlayerState({ status: 'paused' });
          }
        });
      } catch (error) {
        console.error('Audio: Failed to start playback:', error);
        setPlayerState({ status: 'paused' });
      }
    };

    if (status === 'playing' && audio.paused) {
      startPlayback();
    } else if (status === 'paused' && !audio.paused) {
      audio.pause();
    }
  }, [status, isInitialized, currentTrack, initializeAudioContext]);

  // Function to handle track completion
  const handleTrackEnd = useCallback(async () => {
    if (!mountedRef.current || !currentTrack) return;
    
    console.log('AudioSync: Track end detected, handling track completion');
    statusRef.current = 'stopped';
    setPlayerState({ status: 'stopped' });
    trackIdRef.current = null;
    
    // Reset sync state when track ends
    hasInitialSyncRef.current = false;
    positionSyncInProgressRef.current = false;
    bufferingCompensationAppliedRef.current = false;
    
    // We don't need to notify the backend since the server already:
    // 1. Tracks song duration and position internally
    // 2. Advances to next track automatically
    // 3. Broadcasts track changes via SSE to all clients
    console.log('AudioSync: Waiting for server to broadcast next track via SSE');
    
  }, [currentTrack]);
  
  // Add a timer effect to track playback progress and detect end of track
  useEffect(() => {
    if (!currentTrack || !audioRef.current || !isInitialized) return;
    
    // Clear any existing timer
    if (trackEndTimerRef.current) {
      clearInterval(trackEndTimerRef.current);
      trackEndTimerRef.current = null;
    }
    
    const audio = audioRef.current;
    const trackDuration = currentTrack.duration;
    
    // Set up timer to check if we've reached the end of the track
    trackEndTimerRef.current = setInterval(() => {
      if (!audio || audio.paused) return;
      
      // Get current time and compare with duration
      const currentTime = audio.currentTime;
      
      // Add the offset to the current time when comparing with duration
      // This makes the track end timer fire earlier to compensate for delayed playback
      const adjustedCurrentTime = currentTime + TRACK_PLAYBACK_OFFSET;
      const timeRemaining = trackDuration - adjustedCurrentTime;
      
      // Log remaining time for debugging (every 5 seconds)
      if (Math.floor(timeRemaining) % 5 === 0 && timeRemaining < 30) {
        // Only log time remaining at key intervals to reduce verbosity
        console.log(`AudioSync: Track time remaining: ${timeRemaining.toFixed(1)}s`);
      }
      
      // If we're close to the end of the track (within 0.5 second) or passed it
      if (timeRemaining <= 0.5) {
        console.log('AudioSync: Track end timer detected end of track');
        console.log(`AudioSync: Adjusted time: ${adjustedCurrentTime.toFixed(2)}s, Track duration: ${trackDuration.toFixed(2)}s, Remaining: ${timeRemaining.toFixed(2)}s`);
        handleTrackEnd();
        
        // Clear this timer
        if (trackEndTimerRef.current) {
          clearInterval(trackEndTimerRef.current);
          trackEndTimerRef.current = null;
        }
      }
    }, 1000); // Check every second
    
    return () => {
      if (trackEndTimerRef.current) {
        clearInterval(trackEndTimerRef.current);
        trackEndTimerRef.current = null;
      }
    };
  }, [currentTrack, isInitialized, handleTrackEnd]);

  // Initialize singleton audio instance
  useEffect(() => {
    if (!audioRef.current) return;
    
    console.log('Audio: Starting initialization...');
    
    // Flag that we're mounted at the beginning of initialization
    mountedRef.current = true;
    
    // Check if we're on a mobile device right when component mounts
    checkIfMobile();
    
    const initAudio = async () => {
      const audio = audioRef.current;
      if (!audio) return;

      // Only set audio properties if they haven't been set yet
      if (!audio.hasAttribute('initialized')) {
      audio.preload = 'auto';
      audio.setAttribute('playsinline', 'true');
      audio.setAttribute('webkit-playsinline', 'true');
      audio.setAttribute('x-webkit-airplay', 'allow');
        audio.setAttribute('initialized', 'true');
      
      audio.volume = usePlayerStore.getState().volume;
      
      if (globalAudioInstance) {
        audio.src = globalAudioInstance.src;
        audio.currentTime = globalAudioInstance.currentTime;
        audio.volume = globalAudioInstance.volume;
      } else {
        globalAudioInstance = audio;
        }
      }

      // Only attach event listeners once
      if (!audio.hasAttribute('listeners-attached')) {
      const handleError = async (e: Event) => {
        const target = e.target as HTMLAudioElement;
        const error = target?.error;
        console.error('Audio: Playback error:', error?.message);
        
        if (currentTrack && !loadingRef.current) {
          try {
            await setupStream();
            setStreamError(false);
          } catch (recoveryError) {
            console.error('Audio: Stream recovery failed:', recoveryError);
            setStreamError(true);
          }
        } else {
          setStreamError(true);
        }
      };

      audio.addEventListener('error', handleError);
      audio.addEventListener('timeupdate', () => {
        if (mountedRef.current) {
          setPosition(audio.currentTime);
        }
      });

      audio.addEventListener('play', () => {
        if (mountedRef.current) {
          statusRef.current = 'playing';
          setPlayerState({ status: 'playing' });
        }
      });

      audio.addEventListener('pause', () => {
        if (mountedRef.current) {
          statusRef.current = 'paused';
          setPlayerState({ status: 'paused' });
        }
      });

      audio.addEventListener('ended', async () => {
        if (mountedRef.current) {
          console.log('AudioSync: Ended event fired from audio element');
          await handleTrackEnd();
        }
      });

        audio.setAttribute('listeners-attached', 'true');

        // Set up cleanup function
        const cleanup = () => {
        audio.removeEventListener('error', handleError);
      };
        
        return cleanup;
      }
      
      return () => {}; // No cleanup needed if listeners already attached
    };
    
    // Initialize and store cleanup function
    const cleanupPromise = initAudio();
    
    // Set isInitialized to true after audio is set up
    setIsInitialized(true);
    console.log('Audio: Initialization complete');
    
    return () => {
      mountedRef.current = false;
      cleanupPromise.then(cleanupFn => {
        if (typeof cleanupFn === 'function') {
          cleanupFn();
        }
      });
      if (globalAudioInstance === audioRef.current) {
        globalAudioInstance = null;
      }
    };
  }, []); // Only run once on mount

  // Initialize SSE connection - optimized to run only once
  useEffect(() => {
    if (!token || !mountedRef.current || sseSubscriptionRef.current) return;

    console.log('Audio: Setting up SSE subscription');
    const sseManager = SSEManager.getInstance();
    sseManagerRef.current = sseManager;
    sseSubscriptionRef.current = true;

    // Only listen for position updates for audio synchronization
    // Other state updates are handled by the PlayerProvider
    const handlePosition = (data: any) => {
      if (!mountedRef.current || !audioRef.current || !audioRef.current.paused) return;
      audioRef.current.currentTime = data.position;
      setPosition(data.position);
    };

    sseManager.addEventListener('position', handlePosition);

    return () => {
      if (sseSubscriptionRef.current) {
        console.log('Audio: Cleaning up SSE subscription');
      sseSubscriptionRef.current = false;
      sseManager.removeEventListener('position', handlePosition);
      }
    };
  }, [token]); // Only run when token changes

  // Track change effect
  useEffect(() => {
    if (!currentTrack) {
      console.log('AudioSync: No current track available');
      // Reset sync state when we have no track
      hasInitialSyncRef.current = false;
      positionSyncInProgressRef.current = false;
      bufferingCompensationAppliedRef.current = false;
      return;
    }
    
    // Skip if component is not mounted or audio not initialized
    if (!mountedRef.current || !isInitialized) {
      console.log('AudioSync: Component not ready, skipping track change');
      return;
    }
    
    // Skip if already loading
    if (loadingRef.current) {
      console.log('AudioSync: Loading in progress, skipping track change');
      return;
    }
    
    // Skip if track hasn't actually changed
    if (trackIdRef.current === currentTrack.youtubeId) {
      console.log('AudioSync: Track ID unchanged, skipping track change effect');
      return;
    }
    
    console.log('AudioSync: Track change effect triggered', {
      id: currentTrack.youtubeId,
      title: currentTrack.title,
      requestedBy: {
        username: currentTrack.requestedBy?.username,
        avatar: currentTrack.requestedBy?.avatar
      },
      loading: loadingRef.current,
      initialized: isInitialized,
      syncInProgress: positionSyncInProgressRef.current
    });
    
    // Reset sync state for new track
    resetSyncState();
    
    setStreamError(false);
    trackIdRef.current = currentTrack.youtubeId;

    if ('mediaSession' in navigator) {
      // Always use the original youtubeId for artwork
      const originalYoutubeId = currentTrack.youtubeId;
      
      const img = new Image();
      img.onload = () => {
        console.log('AudioSync: Setting media session metadata', {
          title: currentTrack.title,
          artist: currentTrack.requestedBy.username,
          originalId: originalYoutubeId
        });
        
        // Create base URL for artwork using the original youtubeId
        // This is important because the database stores thumbnails using the original ID, not any resolved ID
        const baseUrl = env.apiUrl 
          ? `${env.apiUrl}/api/albumart/${originalYoutubeId}`
          : `/api/albumart/${originalYoutubeId}`;
            
        // For mobile lockscreen players, we need to ensure square images
        // Add a crop parameter to force square aspect ratio
        const artworkUrl = `${baseUrl}?square=1`;
            
        console.log('AudioSync: Using artwork URL:', artworkUrl);

        navigator.mediaSession.metadata = new MediaMetadata({
          title: currentTrack.title,
          artist: currentTrack.requestedBy.username,
          // Provide multiple sizes with the square parameter for different devices
          artwork: [
            { src: artworkUrl, sizes: '96x96', type: 'image/jpeg' },
            { src: artworkUrl, sizes: '128x128', type: 'image/jpeg' },
            { src: artworkUrl, sizes: '192x192', type: 'image/jpeg' },
            { src: artworkUrl, sizes: '256x256', type: 'image/jpeg' },
            { src: artworkUrl, sizes: '384x384', type: 'image/jpeg' },
            { src: artworkUrl, sizes: '512x512', type: 'image/jpeg' }
          ]
        });
        
        // Set media session action handlers
        navigator.mediaSession.setActionHandler('play', () => {
          const audio = document.querySelector('audio');
          if (audio && audio.paused) {
            audio.play().catch(err => console.error('Failed to play:', err));
          }
        });
        
        navigator.mediaSession.setActionHandler('pause', () => {
          const audio = document.querySelector('audio');
          if (audio && !audio.paused) {
            audio.pause();
          }
        });
      };
      
      // Load the image to trigger onload - use the same originalYoutubeId for consistency
      img.src = currentTrack.thumbnail.startsWith('http') 
        ? currentTrack.thumbnail 
        : env.apiUrl 
          ? `${env.apiUrl}/api/albumart/${originalYoutubeId}?square=1`
          : `/api/albumart/${originalYoutubeId}?square=1`;
    }
    
    // Clear any position sync locks before setting up the stream
    positionSyncInProgressRef.current = false;
    
    setupStream().catch(error => {
      console.error('Audio: Failed to setup stream:', error);
      setStreamError(true);
    });
  }, [currentTrack?.youtubeId, isInitialized, setupStream, resetSyncState]);

  // Function to check if we're on a mobile device
  const checkIfMobile = useCallback(() => {
    // Simple mobile detection
    const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
    isMobileDeviceRef.current = isMobile;
    console.log(`AudioSync: Running on ${isMobile ? 'mobile' : 'desktop'} device`);
    return isMobile;
  }, []);

  // Function to correct drift with tiered approach
  const correctDrift = useCallback(async (audio: HTMLAudioElement, serverPosition: number, currentPosition: number) => {
    if (isApplyingCorrectionRef.current || !audio || audio.paused) {
      return;
    }

    const isMobile = isMobileDeviceRef.current;
    
    // Check if we're in the cooldown period on mobile
    const now = Date.now();
    if (isMobile) {
      const timeSinceLastCorrection = now - lastCorrectionTimeRef.current;
      if (timeSinceLastCorrection < DRIFT_SETTINGS.mobile.correctionCooldown) {
        // Skip correction during cooldown period on mobile
        return;
      }
    }

    // Calculate drift (positive = client ahead, negative = client behind)
    // Apply offset to server position for drift calculation
    const adjustedServerPosition = Math.max(0, serverPosition - TRACK_PLAYBACK_OFFSET);
    const drift = currentPosition - adjustedServerPosition;
    const absDrift = Math.abs(drift);
    
    // Add to drift history (store last 5 measurements)
    driftHistoryRef.current.push({timestamp: now, drift});
    if (driftHistoryRef.current.length > 5) {
      driftHistoryRef.current.shift();
    }
    
    // Determine minimum drift threshold based on device
    const minDriftThreshold = isMobile ? 
      DRIFT_SETTINGS.mobile.minDriftThreshold / 1000 : 
      DRIFT_SETTINGS.minDriftThreshold / 1000;
    
    // Only log drift if it's significant or for occasional status updates
    const shouldLogDrift = absDrift >= minDriftThreshold || Math.random() < 0.1; // Log significant drift or ~10% of checks
    if (shouldLogDrift) {
      console.log(`AudioSync: Measured drift: ${drift.toFixed(3)}s (${drift > 0 ? 'ahead' : 'behind'})`);
    }
    
    // Skip correction if drift is below the threshold for the current device
    if (absDrift < minDriftThreshold) {
      if (shouldLogDrift) {
        console.log('AudioSync: Drift within acceptable range, no correction needed');
      }
      return;
    }
    
    // Update the last correction time
    lastCorrectionTimeRef.current = now;
    
    // Set correction flag
    isApplyingCorrectionRef.current = true;
    
    try {
      // TIER 1: Small drift correction via playback rate adjustment (< 300ms)
      if (absDrift < 0.3) {
        console.log('AudioSync: Applying TIER 1 correction (playback rate adjustment)');
        
        // Store original playback rate if not already stored
        if (originalPlaybackRateRef.current === 1) {
          originalPlaybackRateRef.current = audio.playbackRate;
        }
        
        // Calculate adjustment factor (0.95 to 1.05 range)
        // Larger drift = more aggressive correction
        // More gentle for mobile devices
        const adjustmentRange = isMobile ? 0.03 : 0.05; // 3% vs 5% adjustment
        
        const adjustmentFactor = drift > 0 
          ? Math.max(1 - adjustmentRange, 1 - absDrift/6) // Playing ahead, slow down
          : Math.min(1 + adjustmentRange, 1 + absDrift/6); // Playing behind, speed up
          
        // Apply the adjusted playback rate
        audio.playbackRate = adjustmentFactor;
        
        console.log(`AudioSync: Adjusted playback rate to ${adjustmentFactor.toFixed(3)}x to correct ${drift.toFixed(3)}s drift`);
        
        // Schedule restoration of normal playback rate after correction period
        // Calculate time needed for drift correction based on the adjustment factor
        const correctionDuration = absDrift / Math.abs(1 - adjustmentFactor);
        const correctionTime = Math.min(10000, correctionDuration * 1000); // Cap at 10 seconds
        
        console.log(`AudioSync: Will restore normal playback in ${(correctionTime/1000).toFixed(1)}s`);
        
        setTimeout(() => {
          if (audio && !audio.paused) {
            audio.playbackRate = originalPlaybackRateRef.current;
            console.log('AudioSync: Restored original playback rate');
          }
          isApplyingCorrectionRef.current = false;
          originalPlaybackRateRef.current = 1;
        }, correctionTime);
      }
      
      // TIER 2: Medium drift correction via crossfade (300ms - 1.5s)
      // Skip crossfade on mobile if configured to do so
      else if (absDrift < 1.5 && !(isMobile && DRIFT_SETTINGS.mobile.disableCrossfade)) {
        console.log('AudioSync: Applying TIER 2 correction (crossfade)');
        
        // Create crossfade audio element if needed
        if (!crossfadeAudioRef.current) {
          crossfadeAudioRef.current = new Audio();
          crossfadeAudioRef.current.volume = 0;
        }
        
        const crossfadeAudio = crossfadeAudioRef.current;
        
        // Set up crossfade audio with same source
        crossfadeAudio.src = audio.src;
        // Apply offset to server position when setting crossfade time
        crossfadeAudio.currentTime = adjustedServerPosition;
        
        // Wait for crossfade audio to be ready
        await new Promise<void>((resolve) => {
          const canPlayHandler = () => {
            crossfadeAudio.removeEventListener('canplay', canPlayHandler);
            resolve();
          };
          
          crossfadeAudio.addEventListener('canplay', canPlayHandler, { once: true });
          crossfadeAudio.load();
        });
        
        // Start playing the crossfade audio (muted initially)
        await crossfadeAudio.play();
        
        // Perform crossfade over 300ms
        const crossfadeDuration = 300; // ms
        const startTime = performance.now();
        const fadeInterval = setInterval(() => {
          const elapsed = performance.now() - startTime;
          const progress = Math.min(1, elapsed / crossfadeDuration);
          
          // Fade out original audio, fade in crossfade audio
          if (audio && crossfadeAudio) {
            audio.volume = Math.max(0, 1 - progress) * usePlayerStore.getState().volume;
            crossfadeAudio.volume = Math.min(1, progress) * usePlayerStore.getState().volume;
          }
          
          // When crossfade complete, switch to crossfade audio position and restore original audio
          if (progress >= 1) {
            clearInterval(fadeInterval);
            
            if (audio && crossfadeAudio) {
              audio.currentTime = crossfadeAudio.currentTime;
              audio.volume = usePlayerStore.getState().volume;
              crossfadeAudio.pause();
              crossfadeAudio.src = '';
            }
            
            console.log(`AudioSync: Crossfade complete, corrected ${drift.toFixed(3)}s drift`);
            isApplyingCorrectionRef.current = false;
          }
        }, 16); // ~60fps
      }
      
      // TIER 3: Large drift correction via immediate seek (> 1.5s)
      // Or if we're on mobile and crossfade is disabled
      else {
        console.log('AudioSync: Applying TIER 3 correction (immediate seek)');
        
        // For very large drifts, just seek immediately
        audio.currentTime = adjustedServerPosition;
        console.log(`AudioSync: Performed immediate seek to correct ${drift.toFixed(3)}s drift`);
        
        isApplyingCorrectionRef.current = false;
      }
    } catch (error) {
      console.error('AudioSync: Error during drift correction:', error);
      isApplyingCorrectionRef.current = false;
    }
  }, []);

  // Add a periodic drift check effect
  useEffect(() => {
    // Don't run drift checks if not initialized or no track
    if (!isInitialized || !currentTrack || !audioRef.current) {
      return;
    }
    
    // Check if we're on a mobile device
    const isMobile = checkIfMobile();
    
    const checkDrift = async () => {
      // Skip if audio is paused, not initialized, or correction in progress
      if (!audioRef.current || audioRef.current.paused || isApplyingCorrectionRef.current) {
        return;
      }
      
      try {
        // Only do a server check if we haven't done one recently
        // Use longer interval for mobile
        const now = Date.now();
        const timeSinceLastSync = now - lastSyncTimeRef.current;
        const minTimeBetweenChecks = isMobile 
          ? DRIFT_SETTINGS.mobile.minTimeBetweenChecks 
          : 30000; // 30 seconds for desktop
        
        // Get accurate server position
        let serverPosition;
        
        if (timeSinceLastSync > minTimeBetweenChecks) {
          // Do a full server sync periodically
          const response = await fetch(`${env.apiUrl}/api/music/position`);
          const data = await response.json();
          
          // Compensate for network latency (calculate based on request duration)
          const requestDuration = Date.now() - now;
          serverPosition = Math.max(0, data.position + (requestDuration / 1000));
          
          // Update our reference points
          lastSyncTimeRef.current = now;
          serverPositionRef.current = serverPosition;
          initialSyncTimeRef.current = performance.now();
          
          console.log('AudioSync: Periodic full server position check:', serverPosition);
        } else {
          // Use calculated position for more frequent checks
          const elapsedSinceSync = (performance.now() - initialSyncTimeRef.current) / 1000;
          serverPosition = serverPositionRef.current + elapsedSinceSync;
        }
        
        // Get current client-side position
        const currentPosition = audioRef.current.currentTime;
        
        // Apply correction if needed
        correctDrift(audioRef.current, serverPosition, currentPosition);
        
      } catch (error) {
        console.error('AudioSync: Error checking drift:', error);
      }
    };
    
    // Set up interval for checking drift - less frequent checks on mobile
    // and more frequent checks for longer songs
    const baseInterval = isMobile ? 15000 : 10000; // 15 sec for mobile, 10 sec for desktop
    const checkInterval = currentTrack.duration > 180 ? 
      Math.max(5000, baseInterval / 2) : // For long songs, faster but not too fast
      baseInterval;
    
    // Clear any existing interval
    if (driftCheckIntervalRef.current) {
      clearInterval(driftCheckIntervalRef.current);
    }
    
    // Set up new interval
    driftCheckIntervalRef.current = setInterval(checkDrift, checkInterval);
    console.log(`AudioSync: Set up drift checks every ${checkInterval/1000}s`);
    
    // Run an initial check after a longer delay on mobile (to allow for initial sync)
    const initialCheckDelay = isMobile ? 6000 : 3000;
    setTimeout(checkDrift, initialCheckDelay);
    
    return () => {
      if (driftCheckIntervalRef.current) {
        clearInterval(driftCheckIntervalRef.current);
        driftCheckIntervalRef.current = null;
      }
    };
  }, [isInitialized, currentTrack, correctDrift, checkIfMobile]);

  // User activity detection for opportunistic sync
  useEffect(() => {
    if (!isInitialized || !audioRef.current) return;
    
    const handleUserActivity = () => {
      // Only use this opportunity if we have significant drift
      if (driftHistoryRef.current.length > 0) {
        const latestDrift = driftHistoryRef.current[driftHistoryRef.current.length - 1];
        if (Math.abs(latestDrift.drift) > 0.2 && audioRef.current && !audioRef.current.paused) {
          console.log('AudioSync: User activity detected, using opportunity for sync check');
          
          // Reset the last sync time to force a server check
          lastSyncTimeRef.current = 0;
          
          // Request an immediate drift check in 500ms (after the user interaction completes)
          setTimeout(() => {
            if (driftCheckIntervalRef.current) {
              clearInterval(driftCheckIntervalRef.current);
              driftCheckIntervalRef.current = null;
            }
            
            // Run drift check and then resume normal interval
            const audio = audioRef.current;
            if (audio && !audio.paused) {
              const checkDrift = async () => {
                try {
                  const response = await fetch(`${env.apiUrl}/api/music/position`);
                  const data = await response.json();
                  
                  // Compensate for network latency
                  const requestDuration = performance.now() - requestTimeRef.current;
                  const serverPosition = Math.max(0, data.position - (requestDuration / 1000));
                  
                  // Update our reference points
                  lastSyncTimeRef.current = Date.now();
                  serverPositionRef.current = serverPosition;
                  initialSyncTimeRef.current = performance.now();
                  
                  // Apply correction if needed
                  correctDrift(audio, serverPosition, audio.currentTime);
                  
                  // Resume normal interval checks
                  const checkInterval = currentTrack?.duration && currentTrack.duration > 180 ? 5000 : 10000;
                  driftCheckIntervalRef.current = setInterval(checkDrift, checkInterval);
                } catch (error) {
                  console.error('AudioSync: Error during user activity sync:', error);
                }
              };
              
              checkDrift();
            }
          }, 500);
        }
      }
    };
    
    // Detect various user interactions that are good opportunities for sync
    window.addEventListener('click', handleUserActivity);
    document.addEventListener('visibilitychange', handleUserActivity);
    
    return () => {
      window.removeEventListener('click', handleUserActivity);
      document.removeEventListener('visibilitychange', handleUserActivity);
    };
  }, [isInitialized, currentTrack, correctDrift]);

  // Cleanup effect
  useEffect(() => {
    return () => {
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }
      // Clean up crossfade audio
      if (crossfadeAudioRef.current) {
        crossfadeAudioRef.current.pause();
        crossfadeAudioRef.current.src = '';
        crossfadeAudioRef.current = null;
      }
      // Cleanup HLS
      const hlsManager = HLSManager.getInstance();
      hlsManager.destroy();
    };
  }, []);

  // Function to acquire wake lock to prevent device sleep
  const acquireWakeLock = useCallback(async () => {
    if ('wakeLock' in navigator) {
      try {
        // Release any existing wake lock first
        if (wakeLockRef.current) {
          await wakeLockRef.current.release();
          wakeLockRef.current = null;
        }
        
        // Acquire a new wake lock
        wakeLockRef.current = await navigator.wakeLock.request('screen');
        console.log('AudioSync: Wake Lock acquired to prevent device sleep');
        
        // Add event listener to reacquire wake lock if it's released
        wakeLockRef.current.addEventListener('release', () => {
          console.log('AudioSync: Wake Lock released');
          // Only try to reacquire if we're still playing
          if (statusRef.current === 'playing') {
            acquireWakeLock().catch(error => {
              console.warn('AudioSync: Failed to reacquire Wake Lock:', error);
            });
          }
        });
      } catch (error) {
        console.warn('AudioSync: Failed to acquire Wake Lock:', error);
      }
    } else {
      console.log('AudioSync: Wake Lock API not supported on this device');
    }
  }, []);
  
  // Function to release wake lock
  const releaseWakeLock = useCallback(async () => {
    if (wakeLockRef.current) {
      try {
        await wakeLockRef.current.release();
        wakeLockRef.current = null;
        console.log('AudioSync: Wake Lock released');
      } catch (error) {
        console.warn('AudioSync: Error releasing Wake Lock:', error);
      }
    }
  }, []);
  
  // Wake lock management - optimized to reduce redundant acquisition attempts
  useEffect(() => {
    if (!isInitialized || !audioRef.current) return;
    
    const audio = audioRef.current;
    let wakeLockAcquired = false;
    
    const handlePlay = () => {
      if (!wakeLockAcquired) {
        acquireWakeLock().then(() => {
          wakeLockAcquired = true;
        }).catch(error => {
          console.warn('AudioSync: Play event - Failed to acquire Wake Lock:', error);
        });
      }
    };
    
    const handlePause = () => {
      if (wakeLockAcquired) {
        releaseWakeLock().then(() => {
          wakeLockAcquired = false;
        }).catch(error => {
          console.warn('Audio: Pause event - Failed to release Wake Lock:', error);
        });
      }
    };
    
    // Only attach these listeners once
    if (!audio.hasAttribute('wakelock-listeners')) {
      audio.addEventListener('play', handlePlay);
      audio.addEventListener('pause', handlePause);
      audio.setAttribute('wakelock-listeners', 'true');
    
      // Also manage wake lock based on document visibility
      const handleVisibilityChange = () => {
        if (document.visibilityState === 'visible' && statusRef.current === 'playing' && !wakeLockAcquired) {
          acquireWakeLock().then(() => {
            wakeLockAcquired = true;
          }).catch(console.warn);
        }
      };
    
    document.addEventListener('visibilitychange', handleVisibilityChange);
    
    // Check initial status
    if (statusRef.current === 'playing' && !audio.paused && !wakeLockAcquired) {
      acquireWakeLock().then(() => {
        wakeLockAcquired = true;
      }).catch(console.warn);
    }
    
    return () => {
      audio.removeEventListener('play', handlePlay);
      audio.removeEventListener('pause', handlePause);
      document.removeEventListener('visibilitychange', handleVisibilityChange);
      if (wakeLockAcquired) {
        releaseWakeLock().catch(console.warn);
      }
    };
  }, [isInitialized, acquireWakeLock, releaseWakeLock]);

  return (
    <audio 
      ref={audioRef}
      onError={(e) => {
        const target = e.target as HTMLAudioElement;
        const error = target?.error;
        console.error('Audio: Element error:', error?.message);
        setStreamError(true);
      }}
      onLoadStart={() => console.log('Audio: Load started')}
      onLoadedData={() => {
        console.log('Audio: Data loaded');
        
        // Skip position sync if one is already in progress
        if (positionSyncInProgressRef.current) {
          console.log('Audio: Position sync already in progress, skipping sync on data load');
          return;
        }
        
        // This is the perfect time to sync position - audio data is loaded but playback hasn't started
        if (audioRef.current && !hasInitialSyncRef.current && statusRef.current === 'playing') {
          console.log('Audio: First sync - fetching position from server');
          fetchPositionWithLatencyCompensation(audioRef.current).catch(error => {
            console.error('Audio: Failed to fetch position:', error);
          });
        } else if (audioRef.current && hasInitialSyncRef.current && statusRef.current === 'playing') {
          // If we already have sync data, use calculated position
          const calculatedPosition = getCalculatedPosition();
          console.log('Audio: Using calculated position from prior sync:', calculatedPosition);
          setPositionWithBufferingCompensation(audioRef.current, calculatedPosition);
        } else {
          console.log('Audio: Data loaded but not syncing position - current status:', statusRef.current);
        }
      }}
      onCanPlay={() => console.log('Audio: Can play')}
      onPlaying={() => console.log('Audio: Playing')}
      playsInline
      webkit-playsinline="true"
      preload="auto"
    />
  );
} 